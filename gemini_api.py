from google import genai
from dotenv import dotenv_values
import PIL.Image
import numpy as np
import time
import google.api_core.exceptions
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer

class ModelApi():
    def __init__(self, init_hf=False):
        # Initialize the Gemini API client
        config = dotenv_values(".env")
        self.__client = genai.Client(api_key=config.get("API_KEY"))
        
        if init_hf:
            # Initialize the Hugging Face model, processor, and tokenizer
            self.hf_model = VisionEncoderDecoderModel.from_pretrained("ydshieh/vit-gpt2-coco-en")
            self.processor = ViTImageProcessor.from_pretrained("ydshieh/vit-gpt2-coco-en")
            self.tokenizer = AutoTokenizer.from_pretrained("ydshieh/vit-gpt2-coco-en") 

    def textQuery(self, text="Explain how AI works"):
        response = self.__client.models.generate_content(
            model="gemini-2.0-flash",
            contents=text,
        )
        print(response.text)
        return response.text

    def imageQuery(self, image_path, prompt):
        """
        Processes an image and generates a text response based on a prompt using the Gemini model.
        Args:
            image_path: Path to the image file.
            prompt: A text prompt that guides the model's response.
                    Example: "Describe what is in this image." or
                             "What are the key elements in this image?"
        Returns:
            The text response generated by the Gemini model. Returns None if there's an error.
        """
        try:
            img = PIL.Image.open(image_path)
        except FileNotFoundError:
            print(f"Error: Image file not found at {image_path}")
            return None
        except Exception as e:
            print(f"Error opening emage: {e}")
            return None
        time.sleep(4)

        try:
            response = self.__client.models.generate_content(
                model="gemini-2.0-flash",
                contents=[prompt, img]
            )
            print(response.text)
            return response.text
        except google.api_core.exceptions.ResourceExhausted as e:
            print("‚õî Rate limit hit (429). Try again later.")
            return None
        except google.api_core.exceptions.ServiceUnavailable as e:
            print("üö´ Model is overloaded (503), retrying in 10s...")
            return self.imageQuery(image_path, prompt)  # retry once
        except Exception as e:
            print(f"‚ùå Error during image processing: {e}")
            return None

    def huggingfaceQuery(self, image_path, prompt="Generate a short, realistic caption like those in the MS-COCO dataset"):
        """
        Generates a caption using the Hugging Face model.
        Args:
            image_path: Path to the image file.
            prompt: Optional prompt to guide the caption generation.
        Returns:
            The generated caption.
        """
        try:
            # Open the image and preprocess it
            image = PIL.Image.open(image_path).convert("RGB")
            inputs = self.processor(images=image, return_tensors="pt")

            # Generate the caption
            output_ids = self.hf_model.generate(inputs.pixel_values)
            caption = self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
            return caption
        except Exception as e:
            print(f"Error generating Hugging Face caption for {image_path}: {e}")
            return None

